{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.models as models\n",
    "from PIL import Image, ImageFile\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import cv2\n",
    "import gc; gc.enable()\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action=\"ignore\", category=FutureWarning)\n",
    "# keras imports\n",
    "from keras.models import Model\n",
    "from keras.models import Sequential\n",
    "from keras.models import model_from_json\n",
    "from keras.layers import Input\n",
    "# other imports\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "import numpy as np\n",
    "import glob\n",
    "import h5py\n",
    "import json\n",
    "import datetime\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import Callback\n",
    "# Specify title of our final model\n",
    "SAVED_MODEL_NAME = 'effnet_modelB5.h5'\n",
    "def get_preds_and_labels(model, generator):\n",
    "    \"\"\"\n",
    "    Get predictions and labels from the generator\n",
    "    \"\"\"\n",
    "    preds = []\n",
    "    labels = []\n",
    "    for _ in range(int(np.ceil(generator.samples / batch_size))):\n",
    "        x, y = next(generator)\n",
    "        preds.append(model.predict(x))\n",
    "        labels.append(y)\n",
    "    # Flatten list of numpy arrays\n",
    "    return np.concatenate(preds).ravel(), np.concatenate(labels).ravel()\n",
    "\n",
    "class Metrics(Callback):\n",
    "    \"\"\"\n",
    "    A custom Keras callback for saving the best model\n",
    "    according to the Quadratic Weighted Kappa (QWK) metric\n",
    "    \"\"\"\n",
    "    def on_train_begin(self, logs={}):\n",
    "        \"\"\"\n",
    "        Initialize list of QWK scores on validation data\n",
    "        \"\"\"\n",
    "        self.val_kappas = []\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        \"\"\"\n",
    "        Gets QWK score on the validation data\n",
    "        \"\"\"\n",
    "        # Get predictions and convert to integers\n",
    "        y_pred, labels = get_preds_and_labels(model, val_generator)\n",
    "        y_pred = np.rint(y_pred).astype(np.uint8).clip(0, 4)\n",
    "        # We can use sklearns implementation of QWK straight out of the box\n",
    "        # as long as we specify weights as 'quadratic'\n",
    "        _val_kappa = cohen_kappa_score(labels, y_pred, weights='quadratic')\n",
    "        self.val_kappas.append(_val_kappa)\n",
    "        print(f\"val_kappa: {round(_val_kappa, 4)}\")\n",
    "        if _val_kappa == max(self.val_kappas):\n",
    "            print(\"Validation Kappa has improved. Saving model.\")\n",
    "            self.model.save(SAVED_MODEL_NAME)\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = '/Users/flatironschool/Documents/Kaggle/Kaggle-DR-detection/data/'\n",
    "img_dir = train_dir + 'train/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(train_dir+'dr15labels_2.csv')\n",
    "train_df_2 = pd.read_csv('/Users/flatironschool/Documents/Kaggle/Kaggle-DR-detection/drlabels.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = train_df.sample(frac = 0.2)\n",
    "sub2 = train_df_2.sample(frac = 0.2)\n",
    "subs = pd.concat([sub,sub2])\n",
    "val_sub = train_df.sample(frac = 0.05)\n",
    "val_sub2 = train_df_2.sample(frac = 0.05)\n",
    "val_subs = pd.concat([val_sub,val_sub2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "subs.columns = ['id_code','diagnosis']\n",
    "subs['diagnosis'] = subs['diagnosis'].astype(str)\n",
    "for ind in subs.index:\n",
    "    diag = subs['diagnosis'][ind]\n",
    "    subs['id_code'][ind] = img_dir+ str(diag) +'/'+ subs['id_code'][ind] +'.png'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_subs.columns = ['id_code','diagnosis']\n",
    "val_subs['diagnosis'] = val_subs['diagnosis'].astype(str)\n",
    "for ind in val_subs.index:\n",
    "    diag = val_subs['diagnosis'][ind]\n",
    "    val_subs['id_code'][ind] = img_dir+ str(diag) +'/'+ val_subs['id_code'][ind] +'.png'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "xception = keras.applications.Xception(include_top = False, weights = 'imagenet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = xception.output_shape\n",
    "out = xception.output\n",
    "out = keras.layers.GlobalAveragePooling2D()(out)\n",
    "out = keras.layers.Dense(512, activation='relu')(out)\n",
    "total_classes = 5\n",
    "predictions = keras.layers.Dense(total_classes, activation='softmax')(out)\n",
    "\n",
    "model = keras.models.Model(inputs=xception.input, outputs=predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_trainable = False\n",
    "cnt = 0\n",
    "for layer in model.layers:\n",
    "    if cnt>130:\n",
    "        set_trainable = True\n",
    "    if set_trainable:\n",
    "        layer.trainable = True\n",
    "    else:\n",
    "        layer.trainable = False\n",
    "    cnt+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5077 validated image filenames belonging to 5 classes.\n",
      "Found 1418 validated image filenames belonging to 5 classes.\n",
      "WARNING:tensorflow:`epsilon` argument is deprecated and will be removed, use `min_delta` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/flatironschool/.local/lib/python3.6/site-packages/keras_preprocessing/image/dataframe_iterator.py:273: UserWarning: Found 2087 invalid image filename(s) in x_col=\"id_code\". These filename(s) will be ignored.\n",
      "  .format(n_invalid, x_col)\n",
      "/Users/flatironschool/.local/lib/python3.6/site-packages/keras_preprocessing/image/dataframe_iterator.py:273: UserWarning: Found 373 invalid image filename(s) in x_col=\"id_code\". These filename(s) will be ignored.\n",
      "  .format(n_invalid, x_col)\n",
      "WARNING:tensorflow:`epsilon` argument is deprecated and will be removed, use `min_delta` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "45/45 [==============================] - 232s 5s/step - loss: 9.8456\n",
      "val_kappa: 0.2392\n",
      "Validation Kappa has improved. Saving model.\n",
      "159/159 [==============================] - 1557s 10s/step - loss: 0.5337 - val_loss: 9.8456\n",
      "Epoch 2/150\n",
      "45/45 [==============================] - 234s 5s/step - loss: 9.7962\n",
      "val_kappa: 0.2392\n",
      "Validation Kappa has improved. Saving model.\n",
      "159/159 [==============================] - 1502s 9s/step - loss: 0.5293 - val_loss: 9.7962\n",
      "Epoch 3/150\n",
      "45/45 [==============================] - 227s 5s/step - loss: 9.8221\n",
      "val_kappa: 0.2392\n",
      "Validation Kappa has improved. Saving model.\n",
      "159/159 [==============================] - 1476s 9s/step - loss: 0.5159 - val_loss: 9.8221\n",
      "Epoch 4/150\n",
      "45/45 [==============================] - 289s 6s/step - loss: 9.7969\n",
      "val_kappa: 0.2392\n",
      "Validation Kappa has improved. Saving model.\n",
      "159/159 [==============================] - 2929s 18s/step - loss: 0.5078 - val_loss: 9.7969\n",
      "Epoch 5/150\n",
      "45/45 [==============================] - 271s 6s/step - loss: 9.7732\n",
      "val_kappa: 0.2392\n",
      "Validation Kappa has improved. Saving model.\n",
      "159/159 [==============================] - 1793s 11s/step - loss: 0.4914 - val_loss: 9.7732\n",
      "Epoch 6/150\n",
      "45/45 [==============================] - 235s 5s/step - loss: 9.8470\n",
      "val_kappa: 0.2392\n",
      "Validation Kappa has improved. Saving model.\n",
      "159/159 [==============================] - 1652s 10s/step - loss: 0.4945 - val_loss: 9.8470\n",
      "Epoch 7/150\n",
      "45/45 [==============================] - 265s 6s/step - loss: 9.8715\n",
      "val_kappa: 0.2392\n",
      "Validation Kappa has improved. Saving model.\n",
      "159/159 [==============================] - 1690s 11s/step - loss: 0.4790 - val_loss: 9.8715\n",
      "Epoch 8/150\n",
      "45/45 [==============================] - 261s 6s/step - loss: 9.7233\n",
      "val_kappa: 0.2392\n",
      "Validation Kappa has improved. Saving model.\n",
      "159/159 [==============================] - 1752s 11s/step - loss: 0.4645 - val_loss: 9.7233\n",
      "Epoch 9/150\n",
      "45/45 [==============================] - 264s 6s/step - loss: 9.7731\n",
      "val_kappa: 0.2392\n",
      "Validation Kappa has improved. Saving model.\n",
      "159/159 [==============================] - 2771s 17s/step - loss: 0.4540 - val_loss: 9.7731\n",
      "Epoch 10/150\n",
      "45/45 [==============================] - 235s 5s/step - loss: 9.7977\n",
      "val_kappa: 0.2392\n",
      "Validation Kappa has improved. Saving model.\n",
      "159/159 [==============================] - 1547s 10s/step - loss: 0.4496 - val_loss: 9.7977\n",
      "Epoch 11/150\n",
      "45/45 [==============================] - 231s 5s/step - loss: 9.8225\n",
      "val_kappa: 0.2392\n",
      "Validation Kappa has improved. Saving model.\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "159/159 [==============================] - 1505s 9s/step - loss: 0.4572 - val_loss: 9.8225\n",
      "Epoch 12/150\n",
      "45/45 [==============================] - 236s 5s/step - loss: 9.8713\n",
      "val_kappa: 0.2392\n",
      "Validation Kappa has improved. Saving model.\n",
      "159/159 [==============================] - 1517s 10s/step - loss: 0.4356 - val_loss: 9.8713\n",
      "Epoch 13/150\n",
      "45/45 [==============================] - 247s 5s/step - loss: 9.7735\n",
      "val_kappa: 0.2392\n",
      "Validation Kappa has improved. Saving model.\n",
      "159/159 [==============================] - 1708s 11s/step - loss: 0.4375 - val_loss: 9.7735\n",
      "Epoch 14/150\n",
      "45/45 [==============================] - 267s 6s/step - loss: 9.7981\n",
      "val_kappa: 0.2392\n",
      "Validation Kappa has improved. Saving model.\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "159/159 [==============================] - 1695s 11s/step - loss: 0.4262 - val_loss: 9.7981\n",
      "Epoch 15/150\n",
      "45/45 [==============================] - 232s 5s/step - loss: 9.8226\n",
      "val_kappa: 0.2392\n",
      "Validation Kappa has improved. Saving model.\n",
      "159/159 [==============================] - 1640s 10s/step - loss: 0.4213 - val_loss: 9.8226\n",
      "Epoch 16/150\n",
      "45/45 [==============================] - 229s 5s/step - loss: 9.8226\n",
      "val_kappa: 0.2392\n",
      "Validation Kappa has improved. Saving model.\n",
      "159/159 [==============================] - 1505s 9s/step - loss: 0.4182 - val_loss: 9.8226\n",
      "Epoch 17/150\n",
      "45/45 [==============================] - 231s 5s/step - loss: 9.8225\n",
      "val_kappa: 0.2392\n",
      "Validation Kappa has improved. Saving model.\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "159/159 [==============================] - 1493s 9s/step - loss: 0.4188 - val_loss: 9.8225\n",
      "Epoch 18/150\n",
      "45/45 [==============================] - 228s 5s/step - loss: 9.7980\n",
      "val_kappa: 0.2392\n",
      "Validation Kappa has improved. Saving model.\n",
      "159/159 [==============================] - 1498s 9s/step - loss: 0.4058 - val_loss: 9.7980\n",
      "Epoch 19/150\n",
      "45/45 [==============================] - 231s 5s/step - loss: 9.8472\n",
      "val_kappa: 0.2392\n",
      "Validation Kappa has improved. Saving model.\n",
      "159/159 [==============================] - 1503s 9s/step - loss: 0.4110 - val_loss: 9.8472\n",
      "Epoch 20/150\n",
      "45/45 [==============================] - 252s 6s/step - loss: 9.8227\n",
      "val_kappa: 0.2392\n",
      "Validation Kappa has improved. Saving model.\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n",
      "159/159 [==============================] - 1561s 10s/step - loss: 0.4138 - val_loss: 9.8227\n",
      "Epoch 21/150\n",
      "45/45 [==============================] - 234s 5s/step - loss: 9.7734\n",
      "val_kappa: 0.2392\n",
      "Validation Kappa has improved. Saving model.\n",
      "159/159 [==============================] - 1523s 10s/step - loss: 0.4052 - val_loss: 9.7734\n",
      "Epoch 22/150\n",
      "45/45 [==============================] - 231s 5s/step - loss: 9.7980\n",
      "val_kappa: 0.2392\n",
      "Validation Kappa has improved. Saving model.\n",
      "159/159 [==============================] - 1548s 10s/step - loss: 0.4093 - val_loss: 9.7980\n",
      "Epoch 23/150\n",
      "45/45 [==============================] - 229s 5s/step - loss: 9.7734\n",
      "val_kappa: 0.2392\n",
      "Validation Kappa has improved. Saving model.\n",
      "\n",
      "Epoch 00023: ReduceLROnPlateau reducing learning rate to 3.12499992105586e-06.\n",
      "159/159 [==============================] - 1499s 9s/step - loss: 0.4084 - val_loss: 9.7734\n",
      "Epoch 24/150\n",
      "45/45 [==============================] - 23197s 515s/step - loss: 9.7734\n",
      "val_kappa: 0.2392\n",
      "Validation Kappa has improved. Saving model.\n",
      "159/159 [==============================] - 24461s 154s/step - loss: 0.4010 - val_loss: 9.7734\n",
      "Epoch 25/150\n",
      "45/45 [==============================] - 227s 5s/step - loss: 9.8472\n",
      "val_kappa: 0.2392\n",
      "Validation Kappa has improved. Saving model.\n",
      "159/159 [==============================] - 1479s 9s/step - loss: 0.4077 - val_loss: 9.8472\n",
      "Epoch 26/150\n",
      "45/45 [==============================] - 226s 5s/step - loss: 9.7734\n",
      "val_kappa: 0.2392\n",
      "Validation Kappa has improved. Saving model.\n",
      "\n",
      "Epoch 00026: ReduceLROnPlateau reducing learning rate to 1.56249996052793e-06.\n",
      "159/159 [==============================] - 1478s 9s/step - loss: 0.3985 - val_loss: 9.7734\n",
      "Epoch 27/150\n",
      "45/45 [==============================] - 226s 5s/step - loss: 9.7487\n",
      "val_kappa: 0.2392\n",
      "Validation Kappa has improved. Saving model.\n",
      "159/159 [==============================] - 1471s 9s/step - loss: 0.3965 - val_loss: 9.7487\n",
      "Epoch 28/150\n",
      "45/45 [==============================] - 226s 5s/step - loss: 9.8226\n",
      "val_kappa: 0.2392\n",
      "Validation Kappa has improved. Saving model.\n",
      "159/159 [==============================] - 1473s 9s/step - loss: 0.3997 - val_loss: 9.8226\n",
      "Epoch 29/150\n",
      "45/45 [==============================] - 227s 5s/step - loss: 9.8226\n",
      "val_kappa: 0.2392\n",
      "Validation Kappa has improved. Saving model.\n",
      "\n",
      "Epoch 00029: ReduceLROnPlateau reducing learning rate to 7.81249980263965e-07.\n",
      "159/159 [==============================] - 1474s 9s/step - loss: 0.4048 - val_loss: 9.8226\n",
      "Epoch 30/150\n",
      "45/45 [==============================] - 227s 5s/step - loss: 9.7241\n",
      "val_kappa: 0.2392\n",
      "Validation Kappa has improved. Saving model.\n",
      "159/159 [==============================] - 1474s 9s/step - loss: 0.4058 - val_loss: 9.7241\n",
      "Epoch 31/150\n",
      "45/45 [==============================] - 227s 5s/step - loss: 9.8226\n",
      "val_kappa: 0.2392\n",
      "Validation Kappa has improved. Saving model.\n",
      "159/159 [==============================] - 1475s 9s/step - loss: 0.4054 - val_loss: 9.8226\n",
      "Epoch 32/150\n",
      "45/45 [==============================] - 227s 5s/step - loss: 9.8226\n",
      "val_kappa: 0.2392\n",
      "Validation Kappa has improved. Saving model.\n",
      "\n",
      "Epoch 00032: ReduceLROnPlateau reducing learning rate to 3.906249901319825e-07.\n",
      "159/159 [==============================] - 1477s 9s/step - loss: 0.3941 - val_loss: 9.8226\n",
      "Epoch 33/150\n",
      "45/45 [==============================] - 226s 5s/step - loss: 9.7980\n",
      "val_kappa: 0.2392\n",
      "Validation Kappa has improved. Saving model.\n",
      "159/159 [==============================] - 1475s 9s/step - loss: 0.4057 - val_loss: 9.7980\n",
      "Epoch 00033: early stopping\n"
     ]
    }
   ],
   "source": [
    "# training_generator = BalancedBatchGenerator(X_train, y_train,\n",
    "#                                                 batch_size=1000,\n",
    "#                                                 random_state=42)\n",
    "#     model.fit_generator(generator=training_generator, epochs=5, verbose=1)\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "datagen = keras.preprocessing.image.ImageDataGenerator()\n",
    "batch_size = 32\n",
    "train_steps_per_epoch = len(subs)//batch_size\n",
    "adm = tf.keras.optimizers.Adam(lr = 0.0001)\n",
    "\n",
    "train_generator = datagen.flow_from_dataframe(subs, \n",
    "                                              x_col = 'id_code', \n",
    "                                              y_col = 'diagnosis',\n",
    "                                              target_size = (224,224),\n",
    "                                              batch_size = batch_size,\n",
    "                                              class_mode ='categorical') \n",
    "val_generator = datagen.flow_from_dataframe(val_subs, \n",
    "                                            x_col='id_code', \n",
    "                                            y_col='diagnosis',\n",
    "                                            target_size=(224, 224),\n",
    "                                            batch_size=batch_size,\n",
    "                                            class_mode='categorical')\n",
    "# For tracking Quadratic Weighted Kappa score\n",
    "kappa_metrics = Metrics()\n",
    "es = EarlyStopping(monitor='val_loss', mode='auto', verbose=1, patience=25)\n",
    "rlr = ReduceLROnPlateau(monitor='val_loss', \n",
    "                        factor=0.5, \n",
    "                        patience=3, \n",
    "                        verbose=1, \n",
    "                        mode='auto', \n",
    "                        epsilon=0.0001)\n",
    "\n",
    "model.compile(optimizer = adm, loss = 'categorical_crossentropy')\n",
    "history = model.fit_generator(train_generator,\n",
    "                              steps_per_epoch=train_steps_per_epoch, \n",
    "                              validation_data=val_generator,\n",
    "                              validation_steps = val_generator.samples // batch_size,\n",
    "                              epochs=150, verbose=1, callbacks=[kappa_metrics, es, rlr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.savefig('loss_miniXception')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "model_json = model.to_json()\n",
    "with open(\"miniXception.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(\"miniXception.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_last_pretrained_layer = keras.backend.function([model.layers[0].input],\n",
    "                                  [model.layers[131].output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = np.zeros(shape=(21972, 7, 7, 2048))\n",
    "train_labels = np.zeros(shape=(21972,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n",
      "64\n",
      "96\n",
      "128\n",
      "160\n",
      "192\n",
      "224\n",
      "256\n",
      "288\n",
      "320\n",
      "352\n",
      "384\n",
      "416\n",
      "448\n",
      "480\n",
      "512\n",
      "544\n",
      "576\n",
      "608\n",
      "640\n",
      "672\n",
      "704\n",
      "736\n",
      "768\n",
      "800\n",
      "832\n",
      "864\n",
      "896\n",
      "928\n",
      "960\n",
      "992\n",
      "1024\n",
      "1056\n",
      "1088\n",
      "1120\n",
      "1152\n",
      "1184\n",
      "1216\n",
      "1248\n",
      "1280\n",
      "1312\n",
      "1344\n",
      "1376\n",
      "1408\n",
      "1440\n",
      "1472\n",
      "1504\n",
      "1536\n",
      "1568\n",
      "1600\n",
      "1632\n",
      "1664\n",
      "1696\n",
      "1728\n",
      "1760\n",
      "1792\n",
      "1824\n",
      "1856\n",
      "1888\n",
      "1920\n",
      "1952\n",
      "1984\n",
      "2016\n",
      "2048\n",
      "2080\n",
      "2112\n",
      "2144\n",
      "2176\n",
      "2208\n",
      "2240\n",
      "2272\n",
      "2304\n",
      "2336\n",
      "2368\n",
      "2400\n",
      "2432\n",
      "2464\n",
      "2496\n",
      "2528\n",
      "2560\n",
      "2592\n",
      "2624\n",
      "2656\n",
      "2688\n",
      "2720\n",
      "2752\n",
      "2784\n",
      "2816\n",
      "2848\n",
      "2880\n",
      "2912\n",
      "2944\n",
      "2976\n",
      "3008\n",
      "3040\n",
      "3072\n",
      "3104\n",
      "3136\n",
      "3168\n",
      "3200\n",
      "3232\n",
      "3264\n",
      "3296\n",
      "3328\n",
      "3360\n",
      "3392\n",
      "3424\n",
      "3456\n",
      "3488\n",
      "3520\n",
      "3552\n",
      "3584\n",
      "3616\n",
      "3648\n",
      "3680\n",
      "3712\n",
      "3744\n",
      "3776\n",
      "3808\n",
      "3840\n",
      "3872\n",
      "3904\n",
      "3936\n",
      "3968\n",
      "4000\n",
      "4032\n",
      "4064\n",
      "4096\n",
      "4128\n",
      "4160\n",
      "4192\n",
      "4224\n",
      "4256\n",
      "4288\n",
      "4320\n",
      "4352\n",
      "4384\n",
      "4416\n",
      "4448\n",
      "4480\n",
      "4512\n",
      "4544\n",
      "4576\n",
      "4608\n",
      "4640\n",
      "4672\n",
      "4704\n",
      "4736\n",
      "4768\n",
      "4800\n",
      "4832\n",
      "4864\n",
      "4896\n",
      "4928\n",
      "4960\n",
      "4992\n",
      "5024\n",
      "5056\n",
      "5088\n",
      "5120\n",
      "5152\n",
      "5184\n",
      "5216\n",
      "5248\n",
      "5280\n",
      "5312\n",
      "5344\n",
      "5376\n",
      "5408\n",
      "5440\n",
      "5472\n",
      "5504\n",
      "5536\n",
      "5568\n",
      "5600\n",
      "5632\n",
      "5664\n",
      "5696\n",
      "5728\n",
      "5760\n",
      "5792\n",
      "5824\n",
      "5856\n",
      "5888\n",
      "5920\n",
      "5952\n",
      "5984\n",
      "6016\n",
      "6048\n",
      "6080\n",
      "6112\n",
      "6144\n",
      "6176\n",
      "6208\n",
      "6240\n",
      "6272\n",
      "6304\n",
      "6336\n",
      "6368\n",
      "6400\n",
      "6432\n",
      "6464\n",
      "6496\n",
      "6528\n",
      "6560\n",
      "6592\n",
      "6624\n",
      "6656\n",
      "6688\n",
      "6720\n",
      "6752\n",
      "6784\n",
      "6816\n",
      "6848\n",
      "6880\n",
      "6912\n",
      "6944\n",
      "6976\n",
      "7008\n",
      "7040\n",
      "7072\n",
      "7104\n",
      "7136\n",
      "7168\n",
      "7200\n",
      "7232\n",
      "7264\n",
      "7296\n",
      "7328\n",
      "7360\n",
      "7392\n",
      "7424\n",
      "7456\n",
      "7488\n",
      "7520\n",
      "7552\n",
      "7584\n",
      "7616\n",
      "7648\n",
      "7680\n",
      "7712\n",
      "7744\n",
      "7776\n",
      "7808\n",
      "7840\n",
      "7872\n",
      "7904\n",
      "7936\n",
      "7968\n",
      "8000\n",
      "8032\n",
      "8064\n",
      "8096\n",
      "8128\n",
      "8160\n",
      "8192\n",
      "8224\n",
      "8256\n",
      "8288\n",
      "8320\n",
      "8352\n",
      "8384\n",
      "8416\n",
      "8448\n",
      "8480\n",
      "8512\n",
      "8544\n",
      "8576\n",
      "8608\n",
      "8640\n",
      "8672\n",
      "8704\n",
      "8736\n",
      "8768\n",
      "8800\n",
      "8832\n",
      "8864\n",
      "8896\n",
      "8928\n",
      "8960\n",
      "8992\n",
      "9024\n",
      "9056\n",
      "9088\n",
      "9120\n",
      "9152\n",
      "9184\n",
      "9216\n",
      "9248\n",
      "9280\n",
      "9312\n",
      "9344\n",
      "9376\n",
      "9408\n",
      "9440\n",
      "9472\n",
      "9504\n",
      "9536\n",
      "9568\n",
      "9600\n",
      "9632\n",
      "9664\n",
      "9696\n",
      "9728\n",
      "9760\n",
      "9792\n",
      "9824\n",
      "9856\n",
      "9888\n",
      "9920\n",
      "9952\n",
      "9984\n",
      "10016\n",
      "10048\n",
      "10080\n",
      "10112\n",
      "10144\n",
      "10176\n",
      "10208\n",
      "10240\n",
      "10272\n",
      "10304\n",
      "10336\n",
      "10368\n",
      "10400\n",
      "10432\n",
      "10464\n",
      "10496\n",
      "10528\n",
      "10560\n",
      "10592\n",
      "10624\n",
      "10656\n",
      "10688\n",
      "10720\n",
      "10752\n",
      "10784\n",
      "10816\n",
      "10848\n",
      "10880\n",
      "10912\n",
      "10944\n",
      "10976\n",
      "11008\n",
      "11040\n",
      "11072\n",
      "11104\n",
      "11136\n",
      "11168\n",
      "11200\n",
      "11232\n",
      "11264\n",
      "11296\n",
      "11328\n",
      "11360\n",
      "11392\n",
      "11424\n",
      "11456\n",
      "11488\n",
      "11520\n",
      "11552\n",
      "11584\n",
      "11616\n",
      "11648\n",
      "11680\n",
      "11712\n",
      "11744\n",
      "11776\n",
      "11808\n",
      "11840\n",
      "11872\n",
      "11904\n",
      "11936\n",
      "11968\n",
      "12000\n",
      "12032\n",
      "12064\n",
      "12096\n",
      "12128\n",
      "12160\n",
      "12192\n",
      "12224\n",
      "12256\n",
      "12288\n",
      "12320\n",
      "12352\n",
      "12384\n",
      "12416\n",
      "12448\n",
      "12480\n",
      "12512\n",
      "12544\n",
      "12576\n",
      "12608\n",
      "12640\n",
      "12672\n",
      "12704\n",
      "12736\n",
      "12768\n",
      "12800\n",
      "12832\n",
      "12864\n",
      "12896\n",
      "12928\n",
      "12960\n",
      "12992\n",
      "13024\n",
      "13056\n",
      "13088\n",
      "13120\n",
      "13152\n",
      "13184\n",
      "13216\n",
      "13248\n",
      "13280\n",
      "13312\n",
      "13344\n",
      "13376\n",
      "13408\n",
      "13440\n",
      "13472\n",
      "13504\n",
      "13536\n",
      "13568\n",
      "13600\n",
      "13632\n",
      "13664\n",
      "13696\n",
      "13728\n",
      "13760\n",
      "13792\n",
      "13824\n",
      "13856\n",
      "13888\n",
      "13920\n",
      "13952\n",
      "13984\n",
      "14016\n",
      "14048\n",
      "14080\n",
      "14112\n",
      "14144\n",
      "14176\n",
      "14208\n",
      "14240\n",
      "14272\n",
      "14304\n",
      "14336\n",
      "14368\n",
      "14400\n",
      "14432\n",
      "14464\n",
      "14496\n",
      "14528\n",
      "14560\n",
      "14592\n",
      "14624\n",
      "14656\n",
      "14688\n",
      "14720\n",
      "14752\n",
      "14784\n",
      "14816\n",
      "14848\n",
      "14880\n",
      "14912\n",
      "14944\n",
      "14976\n",
      "15008\n",
      "15040\n",
      "15072\n",
      "15104\n",
      "15136\n",
      "15168\n",
      "15200\n",
      "15232\n",
      "15264\n",
      "15296\n",
      "15328\n",
      "15360\n",
      "15392\n",
      "15424\n",
      "15456\n",
      "15488\n",
      "15520\n",
      "15552\n",
      "15584\n",
      "15616\n",
      "15648\n",
      "15680\n",
      "15712\n",
      "15744\n",
      "15776\n",
      "15808\n",
      "15840\n",
      "15872\n",
      "15904\n",
      "15936\n",
      "15968\n",
      "16000\n",
      "16032\n",
      "16064\n",
      "16096\n",
      "16128\n",
      "16160\n",
      "16192\n",
      "16224\n",
      "16256\n",
      "16288\n",
      "16320\n",
      "16352\n",
      "16384\n",
      "16416\n",
      "16448\n",
      "16480\n",
      "16512\n",
      "16544\n",
      "16576\n",
      "16608\n",
      "16640\n",
      "16672\n",
      "16704\n",
      "16736\n",
      "16768\n",
      "16800\n",
      "16832\n",
      "16864\n",
      "16896\n",
      "16928\n",
      "16960\n",
      "16992\n",
      "17024\n",
      "17056\n",
      "17088\n",
      "17120\n",
      "17152\n",
      "17184\n",
      "17216\n",
      "17248\n",
      "17280\n",
      "17312\n",
      "17344\n",
      "17376\n",
      "17408\n",
      "17440\n",
      "17472\n",
      "17504\n",
      "17536\n",
      "17568\n",
      "17600\n",
      "17632\n",
      "17664\n",
      "17696\n",
      "17728\n",
      "17760\n",
      "17792\n",
      "17824\n",
      "17856\n",
      "17888\n",
      "17920\n",
      "17952\n",
      "17984\n",
      "18016\n",
      "18048\n",
      "18080\n",
      "18112\n",
      "18144\n",
      "18176\n",
      "18208\n",
      "18240\n",
      "18272\n",
      "18304\n",
      "18336\n",
      "18368\n",
      "18400\n",
      "18432\n",
      "18464\n",
      "18496\n",
      "18528\n",
      "18560\n",
      "18592\n",
      "18624\n",
      "18656\n",
      "18688\n",
      "18720\n",
      "18752\n",
      "18784\n",
      "18816\n",
      "18848\n",
      "18880\n",
      "18912\n",
      "18944\n",
      "18976\n",
      "19008\n",
      "19040\n",
      "19072\n",
      "19104\n",
      "19136\n",
      "19168\n",
      "19200\n",
      "19232\n",
      "19264\n",
      "19296\n",
      "19328\n",
      "19360\n",
      "19392\n",
      "19424\n",
      "19456\n",
      "19488\n",
      "19520\n",
      "19552\n",
      "19584\n",
      "19616\n",
      "19648\n",
      "19680\n",
      "19712\n",
      "19744\n",
      "19776\n",
      "19808\n",
      "19840\n",
      "19872\n",
      "19904\n",
      "19936\n",
      "19968\n",
      "20000\n",
      "20032\n",
      "20064\n",
      "20096\n",
      "20128\n",
      "20160\n",
      "20192\n",
      "20224\n",
      "20256\n",
      "20288\n",
      "20320\n",
      "20352\n",
      "20384\n",
      "20416\n",
      "20448\n",
      "20480\n",
      "20512\n",
      "20544\n",
      "20576\n",
      "20608\n",
      "20640\n",
      "20672\n",
      "20704\n",
      "20736\n",
      "20768\n",
      "20800\n",
      "20832\n",
      "20864\n",
      "20896\n",
      "20928\n",
      "20960\n",
      "20992\n",
      "21024\n",
      "21056\n",
      "21088\n",
      "21120\n",
      "21152\n",
      "21184\n",
      "21216\n",
      "21248\n",
      "21280\n",
      "21312\n",
      "21344\n",
      "21376\n",
      "21408\n",
      "21440\n",
      "21472\n",
      "21504\n",
      "21536\n",
      "21568\n",
      "21600\n",
      "21632\n",
      "21664\n",
      "21696\n",
      "21728\n",
      "21760\n",
      "21792\n",
      "21812\n",
      "21844\n",
      "21876\n",
      "21908\n",
      "21940\n",
      "21972\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for inputs_batch, labels_batch in train_generator:\n",
    "    features_batch = get_last_pretrained_layer(inputs_batch)[0]\n",
    "    train_features[i : i + len(inputs_batch)] = features_batch\n",
    "    train_labels[i : i +len(inputs_batch)] = labels_batch\n",
    "    i += len(inputs_batch)\n",
    "    print(i)\n",
    "    if i+2 > len(train_features):\n",
    "        break\n",
    "         \n",
    "train_features = np.reshape(train_features, (21972, 7 * 7 * 2048))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3625"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc; gc.enable()\n",
    "del model, xception, train_generator; gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_features' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-8a45a10a10b9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mfeat_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeat_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'train_features' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "feat_train, feat_test, labels_train, labels_test = train_test_split(train_features, train_labels, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oof_preds = np.zeros(train.shape[0])\n",
    "sub_preds = np.zeros(test.shape[0])\n",
    "\n",
    "feature_importance_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "import xgboost as xgb\n",
    "params = {\n",
    "    'objective': 'multi:softprob',  # error evaluation for multiclass training\n",
    "    'num_class': 5,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'folds' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-4726efceb0fb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mn_fold\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtrn_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_idx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfolds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mtrn_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrn_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_cols\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrn_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrn_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mval_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_cols\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mval_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mval_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mgc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'folds' is not defined"
     ]
    }
   ],
   "source": [
    "for n_fold, (trn_idx, val_idx) in enumerate(folds.split(train, y_train)):\n",
    "    \n",
    "    trn_x, trn_y = train[train_cols].iloc[trn_idx], y_train.iloc[trn_idx]\n",
    "    val_x, val_y = train[train_cols].iloc[val_idx], y_train.iloc[val_idx]\n",
    "    gc.collect()\n",
    "    dtrain = xgb.DMatrix(trn_x, trn_y, feature_names=trn_x.columns)\n",
    "    dval = xgb.DMatrix(val_x, val_y, feature_names=val_x.columns)\n",
    "    gc.collect()\n",
    "    \n",
    "    clf = xgb.train(params=params, dtrain=dtrain, num_boost_round=1500, evals=[(dtrain, \"Train\"), (dval, \"Val\")],\n",
    "        verbose_eval= 250, early_stopping_rounds=100) \n",
    "    gc.collect()\n",
    "    \n",
    "    oof_preds[val_idx] = clf.predict(xgb.DMatrix(val_x))\n",
    "    sub_preds += clf.predict(xgb.DMatrix(test[train_cols])) / folds.n_splits\n",
    "    gc.collect()\n",
    "    \n",
    "    xgbfir.saveXgbFI(clf, feature_names=trn_x.columns, OutputXlsxFile='ieee_xgbfir_%sFold.xlsx'%str(n_fold+1), MaxInteractionDepth=9, MaxHistograms=15)\n",
    "    gc.collect()\n",
    "    fold_importance_df = pd.DataFrame()\n",
    "    fold_importance_df[\"feature\"] = pd.DataFrame.from_dict(data=clf.get_fscore(), orient=\"index\", columns=[\"FScore\"])[\"FScore\"].index\n",
    "    fold_importance_df[\"fscore\"] = pd.DataFrame.from_dict(data=clf.get_fscore(), orient=\"index\", columns=[\"FScore\"])[\"FScore\"].values\n",
    "    fold_importance_df[\"fold\"] = n_fold + 1\n",
    "    feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n",
    "    gc.collect()\n",
    "    \n",
    "    print('\\nFold %2d AUC %.6f & std %.6f' %(n_fold + 1, roc_auc_score(val_y, oof_preds[val_idx]), np.std([oof_preds[val_idx]])))\n",
    "    gc.collect()\n",
    "\n",
    "print('\\nCV AUC score %.6f & std %.6f' % (roc_auc_score(y_train, oof_preds), np.std((oof_preds))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'feat_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-5333f9f545e6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeat_train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mbleh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mxgb_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxgb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mXGBClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobjective\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"multi:softprob\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mxgb_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'feat_train' is not defined"
     ]
    }
   ],
   "source": [
    "X = feat_train\n",
    "bleh, y = np.where(labels_train)\n",
    "\n",
    "xgb_model = xgb.XGBClassifier(objective=\"multi:softprob\", random_state=42)\n",
    "xgb_model.fit(X, y)\n",
    "\n",
    "y_pred = xgb_model.predict(feat_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, cohen_kappa_score\n",
    "bleh, y_test = np.where(labels_test)\n",
    "print(cohen_kappa_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
